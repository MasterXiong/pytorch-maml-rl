import numpy as np
import gym

from gym import spaces
from gym.utils import seeding


class Navigation2DEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from 
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal 
    position (ie. the reward is `-distance`). The 2D navigation tasks are 
    generated by sampling goal positions from the uniform distribution 
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic 
        Meta-Learning for Fast Adaptation of Deep Networks", 2017 
        (https://arxiv.org/abs/1703.03400)
    """
    def __init__(self, task={}, goal_sampler=None):
        super(Navigation2DEnv, self).__init__()
        if goal_sampler == 'left':
            self.bound = {'theta_low': 3. * np.pi / 4., 'theta_high': 5. * np.pi / 4.}
        elif goal_sampler == 'right':
            self.bound = {'theta_low': -np.pi / 4., 'theta_high': np.pi / 4.}
        elif goal_sampler == 'up':
            self.bound = {'theta_low': np.pi / 4., 'theta_high': 3. * np.pi / 4.}
        elif goal_sampler == 'bottom':
            self.bound = {'theta_low': -3. * np.pi / 4, 'theta_high': -np.pi / 4.}
        else:
            raise NotImplementedError(goal_sampler)

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
            shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-1., high=1.,
            shape=(2,), dtype=np.float32)

        self._task = task
        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))
        self._state = np.zeros(2, dtype=np.float32)
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def sample_tasks(self, num_tasks):
        goal_theta = self.np_random.uniform(self.bound['theta_low'], self.bound['theta_high'], size=num_tasks)
        goals = np.stack([np.cos(goal_theta), np.sin(goal_theta)], axis=1)
        tasks = [{'goal': goal} for goal in goals]
        return tasks

    def reset_task(self, task):
        self._task = task
        self._goal = task['goal']

    def reset(self, env=True):
        self._state = np.zeros(2, dtype=np.float32)
        return self._state

    def step(self, action):
        action = np.clip(action, -1., 1.)
        assert self.action_space.contains(action)
        self._state = self._state + 0.1 * action

        x = self._state[0] - self._goal[0]
        y = self._state[1] - self._goal[1]
        reward = -np.sqrt(x ** 2 + y ** 2)
        #done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))
        done = False
        info = {
            'task': self._task, 
            'pos': self._state
        }

        return self._state, reward, done, info


class SparseNavigation2DEnv(Navigation2DEnv):
    """ Reward is L2 distance given only within goal radius """

    def __init__(self, task={}, goal_radius=0.2, goal_sampler=None):

        if goal_sampler == 'left':
            self.bound = {'theta_low': 3. * np.pi / 4., 'theta_high': 5. * np.pi / 4.}
        elif goal_sampler == 'right':
            self.bound = {'theta_low': -np.pi / 4., 'theta_high': np.pi / 4.}
        elif goal_sampler == 'up':
            self.bound = {'theta_low': np.pi / 4., 'theta_high': 3. * np.pi / 4.}
        elif goal_sampler == 'bottom':
            self.bound = {'theta_low': -3. * np.pi / 4, 'theta_high': -np.pi / 4.}
        else:
            raise NotImplementedError(goal_sampler)

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
            shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-1., high=1.,
            shape=(2,), dtype=np.float32)

        self._task = task
        self.goal_radius = goal_radius
        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))
        self._state = np.zeros(2, dtype=np.float32)
        self.seed()

    def sparsify_rewards(self, r):
        ''' zero out rewards when outside the goal radius '''
        mask = (r >= -self.goal_radius).astype(np.float32)
        r = r * mask
        return r

    def sample_tasks(self, num_tasks):
        goal_theta = self.np_random.uniform(self.bound['theta_low'], self.bound['theta_high'], size=num_tasks)
        goals = np.stack([np.cos(goal_theta), np.sin(goal_theta)], axis=1)
        tasks = [{'goal': goal} for goal in goals]
        return tasks

    def step(self, action):
        ob, reward, done, d = super().step(action)
        sparse_reward = self.sparsify_rewards(reward)
        # make sparse rewards positive
        if reward >= -self.goal_radius:
            sparse_reward += 1
        d.update({'sparse_reward': sparse_reward})
        d.update({'dense_reward': reward})
        return ob, sparse_reward, done, d